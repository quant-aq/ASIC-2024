{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to ASIC 2024\n",
    "\n",
    "We are now at the live demo portion of the tutorial. You can either clone (or download) this notebook file from the tutorial GitHub repository or follow along in your own environment. At this point, we have already walked through how to install `atmospy` and its dependencies.\n",
    "\n",
    "**Note: no need to take notes on this. This entire notebook has been uploaded to the ASIC-2024 GitHub repository.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atmospy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# disable warnings for demo purposes\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "atmospy.set_theme()\n",
    "\n",
    "# this tutorial was completed using version:\n",
    "atmospy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "When working with air sensor data, you will almost always want to be working with data in the form of a DataFrame. Likely, this means you will be using the `pandas` library to do so. A DataFrame holds 2-dimensional data, consisting of columns and rows, much like a spreadsheet. `atmospy` assumes all data is a DataFrame and doesn't currently support anything else.\n",
    "\n",
    "## Formatting your data to be atmospy compliant\n",
    "\n",
    "While I tried not to make too many assumptions about data types or structure, in order to make the librar functional, `atmospy` makes a couple of key assumptions about data structure which you must follow in order to use the library as intended:\n",
    "\n",
    "  1. Your data must be a `DataFrame`\n",
    "  2. If you are making a figure that requires a timestamp, your column containing the timestamp must be a proper datetime object (not a string)\n",
    "  3. Any columns containing your 'values' must be of numeric type\n",
    "\n",
    "\n",
    "If you aren't familiar with `DataFrames`, I highly recommend getting your hands on Wes McKinney's book on **Python for Data Analysis**. Below, we'll cover a few tricks for ensuring your data is in proper format in the event you aren't familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with timestamps\n",
    "\n",
    "If you've ever worked with data, you will know there are *a ton* of different timestamp formats you may encounter. Pandas (and python generally) have some useful tools for converting a string-formatted timestamp (e.g., \"2024-01-01 01:23:45\") to a proper python datetime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstamp_as_string = \"2024-01-01 01:23:45\"\n",
    "\n",
    "type(tstamp_as_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this to a datetime object using the `to_datetime` function in `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstamp = pd.to_datetime(tstamp_as_string)\n",
    "\n",
    "type(tstamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy enough! Now, how do you know what type your data is? You can inspect the DataFrame quite easily using the `info()` method on a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    columns=['timestamp', 'A', 'B'], \n",
    "    data=[[\"2024-01-01\", 1, 2], [\"2024-01-02\", \"2\", 3]]\n",
    ")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a DataFrame (not a very interesting one, but we're just trying to showcase something). By inspecting the DataFrame, we can see that we have one column - the column that contains a timestamp - that is an object while the other two are integers. Using the `to_datetime` method we looked at above, let's convert our `timestamp` column to a real live timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"timestamp\"] = df[\"timestamp\"].map(pd.to_datetime)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see by looking at the data type, we now have a true datetime object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with non-numeric types\n",
    "\n",
    "Now, what about the two data columns (\"A\" and \"B\"). While column \"B\" is already numeric (it's an integer), column \"A\" isn't! Why not? Well, if you noticed above when we created this (very fake) DataFrame, I explicitly forced one of the numbers to be a string by including quotes around it. While you may think this is rare and/or not something you will encounter in the real world, I can absolutely assure you that you are wrong. It is *very* common that when working with reference data, otherwise numeric columns will include symbols or other non-numeric characters to signify calibration periods or other downtime. \n",
    "\n",
    "While the manufacturer's of these instruments may be breaking every data recording law on the books (not a real thing), it very much happens and can be frustrating to deal with. Fortunately, `pandas` has a fairly robust way of dealing with it. You can force a column to be numeric and tell it what you want to do with values that it can't force to a numeric type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"A\"] = pd.to_numeric(df[\"A\"], errors='coerce')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the `to_numeric` function available in `pandas` and told it to *coerce* any values that were invalid to be NaN. While this didn't actually happen in this very small dataset, it can be useful in real-life scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I load my data?\n",
    "\n",
    "So, how do you actually load a csv (or other) file into Python and get it to be a DataFrame? Well, there are quite a few ways; however, i'm going to show you the easiest one that uses only dependencies you already have installed by function of this library.\n",
    "\n",
    "The `pandas` library has incredibly robust utility functions for loading data from csv, text file, excel, or more. At it's simplist, you can run the single line below and load your file directly into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"<path-to-file-here>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not going to spend too much time on this during this tutorial, so we are going to move on to discussing DataFrame structure, but if you have questions or need additional resources, my two favorite libraries for loading data are `pandas` and `dask`, which is especially useful if working with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-form vs wide-Form data\n",
    "\n",
    "**Note: In order to explain what I mean in this next section, I am going to load an example dataset, even though we won't learn how to do that for a few more minutes.**\n",
    "\n",
    "There are multiple ways in which you can organize a DataFrame (or spreadsheet or database) and the best way really depends on what you're trying to do. There are two common ways to organize a DataFrame:\n",
    "\n",
    "1. **Wide Form**. Where columns and rows contain levels of different variables, much like reading a spreadsheet.\n",
    "1. **Long Form**. Where each variable gets its own column and each observation is a row.\n",
    "\n",
    "\n",
    "For a much more throughout explanation of wide vs. long form data, check out Michael Waskom's description [here](https://seaborn.pydata.org/tutorial/data_structure.html#long-form-vs-wide-form-data) or Hadley Wickham's paper [here](https://vita.had.co.nz/papers/tidy-data.pdf).\n",
    "\n",
    "Rather than try to continue explaining this in words, I will just show you. To do this, I will enlist the help of one of `atmospy`'s example datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block is a last resort in the event internet is not functioning\n",
    "\n",
    "# bc = pd.read_csv(atmospy.utils.get_data_home() + \"/us-bc.csv\")\n",
    "# bc[\"Timestamp GMT\"] = bc[\"Timestamp GMT\"].map(pd.to_datetime)\n",
    "# bc[\"Timestamp Local\"] = bc.apply(\n",
    "#             lambda x: x[\"Timestamp GMT\"] + pd.Timedelta(hours=x[\"GMT Offset\"]), axis=1)\n",
    "# bc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = atmospy.load_dataset(\"air-sensors-pm\")\n",
    "\n",
    "# keep only the first few records\n",
    "df = df.head(15)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we just loaded an example dataset that contains co-located air sensors (Sensor A, Sensor B, and Sensor C) with an EPA FEM monitor (Reference). By inspecting the data, we can see that we have hourly resolution data and for now, I'm only looking at the first 15h of data. Depending on how you do your analysis, this is a very real spreadsheet that you may have and can read into Python. This is **wide-form** data. For each timestamp, we have four observations; however, they're all in the same row! This can be super useful if we wanted to explore the pairwise relationship between all sensors AND reference monitor. But, what if we wanted to easily plot each sensor against the reference, but not each other?\n",
    "\n",
    "We can convert this to **long-form** data by using `pandas`' `melt` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.melt(id_vars=[\"timestamp\", \"Reference\"], value_name=\"Sensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I didn't choose particularly useful column names, you can see that we now have a long-form dataframe where each observation includes both the Reference value (\"Reference\") AND the Sensor value (\"Sensor\") with the identifier for the sensor in the \"variable\" column.\n",
    "\n",
    "I know this is a very short (hopefully not useless) primer on data formatting and types, it will be highly relevant when it comes to actually plotting your data, which we're about to (finally) get to. There are a ton of great resources available for learning more about this subject, two of which are already listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Datasets\n",
    "\n",
    "In order to actually make plots, we need data. While you can easily make synthetic datasets, we've included a few real datasets that are available for you to use. \n",
    "\n",
    "| Name | Description |\n",
    "|:----:|:------------|\n",
    "| `us-ozone` | Ozone data for 2023 from a subset of all US reference sites, directly from EPA |\n",
    "| `us-bc` | Black Carbon data for 2023 from a subset of all US reference sites, directly from EPA |\n",
    "|`air-sensors-pm`| Co-located data from three PM sensors and a FEM reference monitor for a period of ~8 weeks |\n",
    "|`air-sensors-met`| Data from a QuantAQ MODULAIR at a random location with wind direction and wind speed data |\n",
    "\n",
    "\n",
    "It is likely the available datasets will change over time and there is no guarantee these exact ones will remain. You can always check which ones are available by using the utility function `get_dataset_names`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmospy.get_dataset_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run this for the first time, it will cache it locally on your system so that subsequent calls are much quicker. To load a dataset, you can call `load_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone = atmospy.load_dataset(\"us-ozone\")\n",
    "\n",
    "ozone.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the `us-ozone` dataset that includes ~100 unique locations from across the US from 2023. This data was gathered directly from EPA's website and is all federal reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = atmospy.load_dataset(\"us-bc\")\n",
    "bc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is Black Carbon. Just like ozone, this is data from the EPA for 2023 and was downloaded directly from EPA's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = atmospy.load_dataset(\"air-sensors-pm\")\n",
    "pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is a dataset that includes co-located sensor data with a reference monitor. These are all PM2.5 measurements, FYI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met = atmospy.load_dataset(\"air-sensors-met\")\n",
    "met.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last is another air sensor dataset. This one is a random sensor that just so happens to have wind speed and wind direction with it which makes it useful for some of the figures we'll make. Otherwise, it is an entirely uninteresting dataset.\n",
    "\n",
    "\n",
    "If you have more interesting datasets that you're willing to allow others to use, please feel free to reach out to me and I can add them to the `atmospy-data` repository if that's of mutual interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atmospy\n",
    "\n",
    "## Where does it fit?\n",
    "\n",
    "Like `seaborn`, `atmospy` is a wrapper around `matplotlib` (as well as `seaborn` itself). It is designed to be a tool that you *can* use and it is designed to make common figures that are useful in air sensor work a little bit easier to make. Because it is simply a `matplotlib` wrapper, you can modify the axis object that each function returns and continue to customize these figures in near limitless ways.\n",
    "\n",
    "\n",
    "We are going to walk through each of the fundamental figures that exist in `atmospy` today and talk about the type of story you can tell with each one as well as some of the nuts and bolts of actually using the function and it's abilities. \n",
    "\n",
    "\n",
    " <!-- 1. Go over each figure with defaults to show them off\n",
    " 2. Overview how and why you may need to edit them and/or choose different values\n",
    " 3. Talk about matplotlib and seaborn and where to get more info -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Linear Relationships\n",
    "\n",
    "When working with air sensor data, it is quite often that want, or need, to compare two sensors or one air sensor to a reference monitor. Often, this takes the form of a scatter plot between the two. This is what we refer to as a regression plot. In `atmospy`, we call it a `regplot` and it's as easy as providing the function your input dataframe and defining the x and y columns to use as your variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = atmospy.regplot(\n",
    "    data=pm,\n",
    "    x=\"Reference\", \n",
    "    y=\"Sensor A\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may - or may not - look like what you expected. What is the story we're trying to tell with a simple regression plot? At least in the air sensor context, we are often trying to describe how well two different instruments agree with one another. Often, we use a lienar model to describe this and report the fit of them model to describe the relationship.\n",
    "\n",
    "In the default `regplot` figure, you see the following:\n",
    "  * each pair of observations is shown as a single scatter point. They have white edges so that you can (somewhat) tell the difference between different observations.\n",
    "  * We add a best-fit line (OLS) and label the line with the fit parameters in the legend\n",
    "  * The aspect ratio of the figure is 1 so that you can easily tell if the slope were to be very high or very low\n",
    "  * We don't color by a separate variable (like temperature or humidity) - that's not the story we're trying to tell here and it clutters the figure\n",
    "  * we plot the distribution of each variable on the joint axes. This allows you to easily see what the underlying distribution of data was, which can provide useful context for a fit model\n",
    "  * we don't have too many axis ticks or labels - they're not necessary here. \n",
    "\n",
    "\n",
    "What do you think? What would you do differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally, I don't think there is too much I would change about this figure in order to help it tell the story and convey the information more clearly. However, there are a number of ways you can configure the plot based on your needs.\n",
    "\n",
    "For example, you can pass along keyword arguments to control the look of the joint axes. In this case, maybe we want to add a kernel density estimate to the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = atmospy.regplot(\n",
    "    data=pm,\n",
    "    x=\"Reference\", \n",
    "    y=\"Sensor A\",\n",
    "    marginal_kws={\n",
    "        \"kde\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, maybe we want to change the marker style and color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = atmospy.regplot(\n",
    "    data=pm,\n",
    "    x=\"Reference\", \n",
    "    y=\"Sensor A\",\n",
    "    color=\"g\",\n",
    "    marker='^'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, maybe we don't actually *want* to add the best-fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = atmospy.regplot(\n",
    "    data=pm,\n",
    "    x=\"Reference\", \n",
    "    y=\"Sensor A\",\n",
    "    fit_reg=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is something that isn't possible to modify using the typical arguments of the function, no worries! We can also modify after the fact. Say we want to change the x-axis label to be more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = atmospy.regplot(\n",
    "    data=pm,\n",
    "    x=\"Reference\", \n",
    "    y=\"Sensor A\",\n",
    "    fit_reg=False\n",
    ")\n",
    "g.set_axis_labels(xlabel=\"Teledyne T640 $PM_{2.5}$ [$µgm^{-3}$]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return from the `atmospy` function is just a `matplotlib` Axes object or `seaborn` JointGrid object, so modifying them is quite easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Diel Trends\n",
    "\n",
    "A diel cycle is a pattern that recurs every 24h. We often see diel trends with pollutants that are driven by photochemistry such as ozone, which typically peaks in the early-to-mid afternoon each day and then decreases. We can also find diel patterns associated with human activity such as traffic patterns.\n",
    "\n",
    "To make a diel plot, we typically have to do a good amount of data munging first including determining the mean value at a given time of day, every day, and then grouping those together into a single figure.\n",
    "\n",
    "\n",
    "...or you can use `atmospy.dielplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's limit our data to a single site \n",
    "ozone_single_site = ozone[\n",
    "    ozone[\"Local Site Name\"] == ozone[\"Local Site Name\"].unique()[0]\n",
    "]\n",
    "\n",
    "ax = atmospy.dielplot(\n",
    "    data=ozone_single_site,\n",
    "    x=\"Timestamp Local\",\n",
    "    y=\"Sample Measurement\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we show the figure with the default settings and, well, it leaves a bit to be desired, though it does have the basics. We see the diel trend shown in the darker blue line and the IQR shown in the shaded blue line. Personally, I would like to see the y-axis go to zero, i'd like to see some labels, and i'd like to see the main diel line be a bit darker and more pronounced.\n",
    "\n",
    "Let's go ahead and quickly make those changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = atmospy.dielplot(\n",
    "    data=ozone_single_site,\n",
    "    x=\"Timestamp Local\",\n",
    "    y=\"Sample Measurement\",\n",
    "    ylabel=\"$O_3$ [ppbv]\",\n",
    "    ylim=(0, None),\n",
    "    plot_kws={\n",
    "        \"lw\": 5\n",
    "    }   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit better, though could still be improved a bit. Overall, it tells us what we're looking for. On a repeating, 24h cycle, ozone tends to be lowest around 5 or 6 AM and highest around 3 PM local time. It's not overly complicated and isn't too busy. \n",
    "\n",
    "What information is this figure still missing that might be useful to convey? Maybe some more information about the distribution? Do we really need this to be a continuous line? Or could it be some sort of bar chart that is grouped by hour? \n",
    "\n",
    "What about the information conveyed? How else could we break up, or subselect the data, to be more specific? What if we were plotting CO next to a roadway rather than ozone, which is going to be more regionally consistent?\n",
    "\n",
    "Anyways, more on how we can explore and graph some of those things in a bit.\n",
    "\n",
    "While the diel plot shows you the average (with some statistics) of the 24h cycle, sometimes, we want to look at how the deil cycle changes on a day-to-day basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Short-to-Medium Term Trends\n",
    "\n",
    "We can use a variant of a calendar plot to easily graph this, where we plot the hour of day on one axis and the day of week or day of month on the other axis. In `atmospy`, you can use the `calenderplot` function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = atmospy.calendarplot(\n",
    "    data=ozone_single_site,\n",
    "    x=\"Timestamp Local\",\n",
    "    y=\"Sample Measurement\",\n",
    "    freq=\"hour\",\n",
    "    xlabel=\"Day of Month\",\n",
    "    ylabel=\"Time of Day\",\n",
    "    height=4,\n",
    "    vmin=0, vmax=80,\n",
    "    cbar=False,\n",
    "    cmap=\"flare\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the y-axis here, we go from midnight to midnight top to bottom and on the x-axis we go from the first day of the month to the last. This is a quick and easy way to view quite a bit of data at once - we're showing the mean hourly ozone values in a way that allows your eye to detect subtle trends:\n",
    "\n",
    "  * we have consistent peaks between 1 PM and 6 PM\n",
    "  * we have a couple of days where reported ozone is fairly high even at easlry hours (~4 days in this month?)\n",
    "  * we're missing almost 2 days of data!\n",
    "\n",
    "\n",
    "It would be quite hard to convey this much information with a timeseries, do you agree?\n",
    "\n",
    "By default, we plot the mean value in a given hour. However, you can configure it to plot by any other aggregation as well if you so choose (e.g., max, min, median). Simply change the `agg` argument and you're good to go. We'll show here for completeness, though with ozone, we don't expect much of a difference between mean and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = atmospy.calendarplot(\n",
    "    data=ozone_single_site,\n",
    "    x=\"Timestamp Local\",\n",
    "    y=\"Sample Measurement\",\n",
    "    freq=\"hour\",\n",
    "    xlabel=\"Day of Month\",\n",
    "    ylabel=\"Time of Day\",\n",
    "    height=4,\n",
    "    vmin=0, vmax=80,\n",
    "    cbar=False,\n",
    "    cmap=\"flare\",\n",
    "    agg=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would argue there are better ways to show this in a cleaner and more concise manner, but this is a figure I get asked about a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Long-Term Trends\n",
    "\n",
    "While you could quite easily expand the x-axis above and include all 365 days in a year, do you really need to show the diel trend for an entire year at once? Almost certaintly not...\n",
    "\n",
    "\n",
    "However, it can be very useful to show long-term trends in a pollutant. While there are several good ways to do this, one approach would be to use a `calendarplot` like above, but rather than showing hourly values, we show daily values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmospy.calendarplot(\n",
    "    data=ozone_single_site,\n",
    "    x=\"Timestamp Local\",\n",
    "    y=\"Sample Measurement\",\n",
    "    freq=\"day\",\n",
    "    cbar=False,\n",
    "    height=2,\n",
    "    linewidths=0.1,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lack of data completeness aside from the random ozone monitor we chose aside...\n",
    "\n",
    "\n",
    "In this figure, each column is a week and each row corresponds to a day of the week. Here' we're visualizing the mean ozone value by day over the course of a (albeit stunted) year. \n",
    "\n",
    "What story can you tell with a figure like this? Well, for one, I would bet that if you plotted this for PM2.5 somewhere on the East Coast during 2023, you would mostly see values lower than 15 µg/m3 with the key exception of a couple of weeks where PM2.5 was incredibly high due to wildfires.\n",
    "\n",
    "Next, we're going to move onto the last of our core four figures in `atmospy` before moving on to some more advanced techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualing Pollutant Source\n",
    "\n",
    "Identifying the source of pollution is one of the most common questions we ask. This can be an incredibly challenging task and is tackled in a number of different ways. One way that we can begin to tell a story is by looking at from which direction a pollutant is orgininating. A wind rose is a common figure for depicting the direction and strength of wind. Here, we show a variant of the wind rose called a pollution rose, which is quite similar, though instead of visualizing the wind speed, we're visualizing the pollution intensity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an example dataset with MET info\n",
    "met = atmospy.load_dataset(\"air-sensors-met\")\n",
    "\n",
    "atmospy.pollutionroseplot(\n",
    "    data=met,\n",
    "    ws=\"ws\",\n",
    "    wd=\"wd\",\n",
    "    pollutant=\"pm25\",\n",
    "    calm=0.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used the `pollutionroseplot` available in atmospy to show the directionality and intensity of PM2.5.\n",
    "\n",
    "So what's going on in this figure? We've taken the combination of wind speed, wind direction, and PM2.5 data and binned it both along the theta using wind direction as well as the radius using the PM2.5 value. Both variables are grouped/binned and those values are configurable by you using the `segments` (wind direction) and `bins` (pollutant) arguments.\n",
    "\n",
    "So where does wind speed come in to play? Our goal is to show the direction from where the PM2.5 came. If we are under calm wind conditions, we may not have reason to believe that the wind direction we record is actually true or valid. Therefore, we don't use any of the data that we consider to be under \"calm\" wind conditions. Again, this is configurable by the user using the `calm` argument. We visualize this by leaving the center of the figure blank with an area that is proportional to the percentage of calm winds. To quickly visualize this, let's first set `calm=-10` and then `calm=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmospy.pollutionroseplot(\n",
    "    data=met,\n",
    "    ws=\"ws\",\n",
    "    wd=\"wd\",\n",
    "    pollutant=\"pm25\",\n",
    "    calm=-10.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmospy.pollutionroseplot(\n",
    "    data=met,\n",
    "    ws=\"ws\",\n",
    "    wd=\"wd\",\n",
    "    pollutant=\"pm25\",\n",
    "    calm=10.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes quite clear that this center bit has increased dramatically to convey that a larger percentage of our data occured under 'calm' wind conditions.\n",
    "\n",
    "So how do interpret this figure? To illustrate this, let's make a more final/professional version of this figure and increase the bins and segments to more meaningful values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmospy.pollutionroseplot(\n",
    "    data=met,\n",
    "    ws=\"ws\",\n",
    "    wd=\"wd\",\n",
    "    pollutant=\"pm25\",\n",
    "    calm=0.1,\n",
    "    bins=[0, 8, 15, 25, 35, 50, 100],\n",
    "    segments=32,\n",
    "    suffix=\"$µgm^{-3}$\",\n",
    "    title=\"$PM_{2.5}$ by Direction at an Unknown Location\",\n",
    "    cmap=\"viridis\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now what do we see? It seems to be that at this location, most of the PM2.5 is coming from the south-west and south-east. The longer the bar, the more data records that were grouped with that wind direction. The color of the bar indicates the intensity of the pollutant where the darker colors are higher values in this case. There doesn't seem to be any individual direction that stands out in terms of frequency of high PM2.5 values, which in and of itself is an insight.\n",
    "\n",
    "The data above encompasses something like 7 months of data, so it's quite possible there is some seasonal or time of day influence that is hidden due to the averaging time shown above. We can begin to explore some of those questions in the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "As we've alluded to above, there is quite a bit we can do with the above 4 figures once we begin to subset or divide up our data based on engineered features. It can be incredibly useful to draw the same figure on different subsets of your data, which we refer to as \"Faceting\". We're going to walk through a couple of examples to illustrate (a) how to do this and (b) how powerful or insightful this can be.\n",
    "\n",
    "I appologize in advance that the datasets we have available may not be the most beautiful and/or insightful. The process and idea still holds up, though!\n",
    "\n",
    "\n",
    "**Exploring the influence of traffic on exposure**\n",
    "\n",
    "Imagine you have a sensor along a major roadway and wanted to explore the influence of cars on exposure at this location. One very simple way to visualize this would be to look at the diel trend in CO, NOx, or Black Carbon, but do so separately for weekdays vs weekends when you expect traffic to be greatly reduced.\n",
    "\n",
    "Let's illustrate this using the Black Carbon (`us-bc`) dataset we have available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the dataset and then choose a random location that we can use to illustrate our point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = atmospy.load_dataset(\"us-bc\")\n",
    "\n",
    "# select just one random location for now\n",
    "bc_single_site = bc[bc[\"Local Site Name\"] == bc[\"Local Site Name\"].unique()[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**\n",
    "\n",
    "Next, we want to add a column that identifies the record as taking place on a weekday vs weekend. This is quite easy to do since we already have a column with the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column that sets a bool if the date is a weekend\n",
    "bc_single_site.loc[:, \"Is Weekend\"] = (\n",
    "    bc_single_site[\"Timestamp Local\"].dt.day_name().isin([\"Saturday\", \"Sunday\"])\n",
    ")\n",
    "\n",
    "bc_single_site.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to convert our dataset to be long-form rather than wide-form, which we can do by using the `melt` function in `pandas` that we described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to long-form for faceting\n",
    "bc_long_form = bc_single_site.melt(\n",
    "    id_vars=[\"Timestamp Local\", \"Is Weekend\"], value_vars=[\"Sample Measurement\"]\n",
    ")\n",
    "\n",
    "bc_long_form.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our engineered \"Is Weekend\" column, we can go ahead and set up a `FacetGrid` using `seaborn` and then plot the diel trend on top using the `dielplot` function from `atmospy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    data=bc_long_form,\n",
    "    col=\"Is Weekend\",\n",
    "    # let's adjust the aspect ratio for funsies\n",
    "    aspect=1.25,\n",
    ")\n",
    "g.map_dataframe(\n",
    "    atmospy.dielplot, \n",
    "    x=\"Timestamp Local\", \n",
    "    y=\"value\", \n",
    "    plot_kws=dict(lw=4)\n",
    ")\n",
    "\n",
    "g.set(ylabel=\"Black Carbon\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this may not be the greatest example, you can see a fairly stark difference between weekday and weekend Black Carbon trends at this location. As was printed out above, this site is in Hartford, CT and likely is seeing influence from the nearby roadway. You can see that:\n",
    "\n",
    "  * The diel trend varies quite a bit day-to-day as shown by the large IQR\n",
    "  * Weekdays see higher BC levels than weekends at this location\n",
    "  * On weekdays, there is a pronounced morning rush-hour peak around 7 AM local time\n",
    "\n",
    "Using faceting, we can quickly and clearly show the distinct difference between weekday and weekend exposure for Black Carbon at this location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring Seasonal Differences in PM2.5 Source**\n",
    "\n",
    "Depending on your location, it is quite possible that the source of whatever pollutant you're interested in changes seasonally. There are a number of ways in which you could visualize this, but for the sake of showcasing what you can do with `atmospy`, let's do so by plotting the pollution rose by month.\n",
    "\n",
    "The dataset we have available (`air-sensors-met`) contains data for April through November. Like the previous figure, we're going to set up a `seaborn` `FacetGrid` to plot on. However, we first need to engineer a new column that contains the facet column (here, the month):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the example dataset\n",
    "met = atmospy.load_dataset(\"air-sensors-met\")\n",
    "\n",
    "# add a column that extracts the month from the timestamp_local column\n",
    "met.loc[:, \"Month\"] = met[\"timestamp_local\"].dt.month_name()\n",
    "\n",
    "# print the first 5 records\n",
    "met.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to convert our data to be long-form rather than wide-form, as we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_long_form = met.melt(\n",
    "    id_vars=[\"timestamp_local\", \"Month\", \"ws\", \"wd\"], \n",
    "    value_vars=[\"pm25\"]\n",
    ")\n",
    "\n",
    "# print the first 5 records\n",
    "met_long_form.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we are going to set up a `FacetGrid` and direct it to use the `Month` column to facet on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the FacetGrid\n",
    "g = sns.FacetGrid(\n",
    "    data=met_long_form,\n",
    "    col=\"Month\",\n",
    "    col_wrap=3,\n",
    "    subplot_kws={\"projection\": \"polar\"},\n",
    "    despine=False,\n",
    ")\n",
    "\n",
    "# map the dataframe using the pollutionroseplot function\n",
    "g.map_dataframe(\n",
    "    atmospy.pollutionroseplot,\n",
    "    ws=\"ws\",\n",
    "    wd=\"wd\",\n",
    "    pollutant=\"value\",\n",
    "    faceted=True,\n",
    "    segments=16,\n",
    "    bins=[0, 8, 15, 25, 35, 50, 100],\n",
    "    calm=0.1,\n",
    "    suffix=\"$µgm^{-3}$\",\n",
    ")\n",
    "\n",
    "# add the legend and place it where it looks nice\n",
    "g.add_legend(\n",
    "    title=\"$PM_{2.5}$\", bbox_to_anchor=(0.535, 0.2), handlelength=1, handleheight=1\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting Issues or Feature Requests\n",
    "\n",
    "If you decide to use `atmospy` and notice an issue, please file a bug report on the GitHub repository (show example). If you'd like to see a new feature added, please also report to the same GitHub repository. I can't promise anything, but I'll do my best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping Up\n",
    "\n",
    "That's it for the walk-through. What can you make? Feel free to use your own data or one of the example datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
